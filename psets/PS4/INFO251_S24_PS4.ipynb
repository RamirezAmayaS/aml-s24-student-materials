{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckWwG9pdjl7E"
   },
   "source": [
    "# PS4: Gradient descent and regularization\n",
    "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
    "\n",
    "* Part 1 is meant to be easy, so get through it quickly.\n",
    "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of this problem set so make sure to do it well and understand what you've done. If you find your gradient descent algorithm is taking more than a few minutes to complete, debug more, compare notes with others, and go to the TA sessions (especially the sections on vectorized computation and computational efficiency).\n",
    "* Depending on how well you've done 2.1, parts 2.3 and 4.3 will be relatively painless or incredibly painful. \n",
    "* Part 4 (especially 4.3) will be computationally intensive. Don't leave this until the last minute, otherwise your code might be running when the deadline arrives.\n",
    "* Do the extra credit problems last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Bscj7wWjl7I"
   },
   "source": [
    "---\n",
    "\n",
    "## Introduction to the assignment\n",
    "\n",
    "As with the last assignment, you will be using a modified version of the [California Housing Prices Dataset](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html). Please download the csv file from bcourses ('cal_housing_data_clean_ps4.csv'). \n",
    "\n",
    "To perform any randomized operation, only use functions in the *numpy library (np.random)*. Do not use other packages for random functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0_LwTzXjl7J",
    "jupyter": {
     "outputs_hidden": true
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt  \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72miT9Zhjl7K",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the California Housing Dataset \n",
    "cal_df = pd.read_csv('cal_housing_data_clean_ps4.csv')\n",
    "\n",
    "# leave the following line untouched, it will help ensure that your \"random\" split is the same \"random\" split used by the rest of the class\n",
    "np.random.seed(seed=1948)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cbXmKF9jl7L"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 1: Getting oriented\n",
    "\n",
    "\n",
    "### 1.1 Use existing libraries\n",
    "\n",
    "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
    "\n",
    "Use the [Linear Regression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn or the [OLS class](http://wiki.scipy.org/Cookbook/OLS) from SciPy to explore the relationship between median housing value and median income in California's census block groups. \n",
    "\n",
    "(a) Regress the median housing value `MedHouseVal` on the median income `MedInc`. Draw a scatter plot of housing price (y-axis) against income (x-axis), and draw the regression line in blue.  You might want to make the dots semi-transparent if it improves the presentation of the figure. \n",
    "\n",
    "(b) Regress the median housing value on median income and median income squared.  Plot this new (curved) regression line in gold, on the same axes used for part (a). \n",
    "\n",
    "(c) Interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxCNK0pTjl7L",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjxD1XjXjl7M"
   },
   "source": [
    "*Enter your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkO8pHRvjl7N"
   },
   "source": [
    "### 1.2 Training and testing\n",
    "\n",
    "Chances are, for the above problem you used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but if your primary objective is prediction, you should be careful about overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
    "\n",
    "Using the [cross-validation functions](http://scikit-learn.org/stable/modules/cross_validation.html) from scikit-learn, use 5-fold cross-validation to fit the regression model (a) from 1.1, i.e. the linear fit of median housing value on median income. Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Create a new scatterplot of housing price against income, and draw the five different regression lines in light blue, and the original regression line from 1.1 in red (which was estimated using the full dataset). What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnVo_0TDjl7O",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9CI66kKjl7O"
   },
   "source": [
    "*Enter your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cblgmgiBjl7P"
   },
   "source": [
    "# Part 2: Gradient descent: Linear Regression\n",
    "\n",
    "This is where it gets fun!\n",
    "\n",
    "### 2.1 Implement gradient descent with one independent variable (median income)\n",
    "\n",
    "Implement the batch gradient descent algorithm that we discussed in class. Use the version you implement to regress the median house value on the median income. Experiment with 3-4 different values of the learning rate *R*, and do the following:\n",
    "\n",
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge (for each value of *R*)\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "* How do your coefficients compare to the ones estimated through standard libraries in 1.1? Does this depend on *R*?\n",
    "\n",
    "Some skeleton code is provided below, but you should feel free to delete this code and start from scratch if you prefer.\n",
    "\n",
    "* *Hint 1: Don't forget to implement a stopping condition, so that at every iteration you check whether your results have converged. Common approaches to this are to (a) check to see if the loss has stopped decreasing; and (b) check if both your current parameter esimates are close to the estimates from the previous iteration.  In both cases, \"close\" should not be ==0, it should be <=epsilon, where epsilon is something very small (like 0.0001).*\n",
    "* *Hint 2: We recommend including a MaxIterations parameter in their gradient descent algorithm, to make sure things don't go off the rails, i.e., as a safeguard in case your algorithm isn't converging as it should. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAfAQ5EQjl7Q",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "bivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find coefficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta: float\n",
    "    coefficient\n",
    "\"\"\"\n",
    "def bivariate_ols(xvalues, yvalues, R=0.01, MaxIterations=1000):\n",
    "    start_time = time.time()\n",
    "    #your code here\n",
    "    \n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return alpha, beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7dK9bMZjl7Q"
   },
   "source": [
    "*Enter your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZqg5yW3jl7Q"
   },
   "source": [
    "### 2.2 Data normalization (done for you!)\n",
    "\n",
    "Soon, you will implement a version of gradient descent that can use an arbitrary number of independent variables. Before doing this, we want to give you some code to standardize your features. \n",
    "\n",
    "**For all the following questions, unless explicitly asked otherwise, you are expected to standardize appropriately. Recall that in settings where you are using holdout data for validation or testing purposes, this involves substracting the average and dividing by the standard deviation of your training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU9-kG3Tjl7Q",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function\n",
    "--------\n",
    "standardize\n",
    "    Column-wise standardization of a target dataframe using the mean and std of a reference dataframe\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "ref,tar : pd.DataFrame\n",
    "    ref: reference dataframe\n",
    "    tar: target dataframe\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "tar_norm: pd.DataFrame\n",
    "    Standardized target dataframe\n",
    "'''\n",
    "def standardize(ref,tar):\n",
    "    tar_norm = ((tar - np.mean(ref, axis = 0)) / np.std(ref, axis = 0))\n",
    "    return tar_norm\n",
    "\n",
    "# Examples\n",
    "# Standardize train: standardize(ref=x_train,tar=x_train)\n",
    "# Standardize test: standardize(ref=x_train,tar=x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUUackd0jl7R"
   },
   "source": [
    "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
    "\n",
    "Now that you have a simple version of gradient descent working, create a version of gradient descent that can take more than one independent variable.  Assume all independent variables will be continuous.  Test your algorithm using `MedInc`, `HouseAge` and `AveRooms` as independent variables. Remember to standardize appropriately before inputting them to the gradient descent algorithm. How do your coefficients compare to the ones estimated through standard libraries?\n",
    "\n",
    "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with three values of R (0.1, 0.01, and 0.001).\n",
    "\n",
    "* *Hint 1: Be careful to implement this efficiently, otherwise it might take a long time for your code to run. Commands like `np.dot` can be a good friend to you on this problem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhEcthNtjl7R",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find coefficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalue_matrix, yvalues : narray\n",
    "    xvalue_matrix: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta_array: array[float]\n",
    "    coefficient\n",
    "\"\"\"\n",
    "\n",
    "def multivariate_ols(xvalue_matrix, yvalues, R=0.01, MaxIterations=1000):\n",
    "    start_time = time.time()\n",
    "    # your code here\n",
    "    \n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return alpha, beta_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2o_SDdcjl7R"
   },
   "source": [
    "*Enter your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMXw_qcYjl7S"
   },
   "source": [
    "### 2.4 Compare standardized vs. non-standardized results\n",
    "\n",
    "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original data. Use the same three values of R (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm? Compare to the results you would obtain using standard libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnEvJxxhjl7S",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFdApjEsjl7S"
   },
   "source": [
    "*Enter your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pEk2_UZjl7S"
   },
   "source": [
    "# 3. Prediction\n",
    "\n",
    "Let's use our fitted model to make predictions about housing prices. \n",
    "\n",
    "### 3.1 Cross-Validation\n",
    "\n",
    "Unless you were careful above, you probably overfit your data again. Let's fix that. Use 5-fold cross-validation to re-fit the multivariate regression from 2.3 above, and report your estimated coefficients (there should be four, corresponding to the intercept and the three coefficients for `MedInc` and `AveRoomsNorm`, `HouseAgeNorm`). Since there are 5 folds, there will be 5 sets of four coefficients -- report them all in a 5x4 table.\n",
    "\n",
    "**Note:** You can use KFold to perform the cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGQRERuIjl7T",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_rmse(predictions, yvalues):\n",
    "    P = np.array(predictions)\n",
    "    Y = np.array(yvalues)\n",
    "    rmse = ((P-Y)**2).sum()*1.0 / len(P) \n",
    "    rmse = np.sqrt(rmse)\n",
    "    return rmse\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHk88-z5jl7T"
   },
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaRVr5wpjl7T"
   },
   "source": [
    "### 3.2 Predicted values and RMSE\n",
    "\n",
    "Let's figure out how accurate this predictive model turned out to be. Compute the cross-validated RMSE for each of the 5 folds above. In other words, in fold 1, use the parameters estimated on the 80% of the data to make predictions for the 20%, and calculate the RMSE for those 20%. Repeate this for the remaining folds. Report the RMSE for each of the 5-folds, and the average (mean) RMSE across the five folds. How does this average RMSE compare to the performance of your nearest neighbor algorithm from the last problem set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9I1zFLBajl7T",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61FuGxiYjl7T"
   },
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvkpICWsjl7T"
   },
   "source": [
    "## 4 Regularization \n",
    "\n",
    "### 4.1 Get prepped\n",
    "\n",
    "Step 1: Generate features consisting of all polynomial combinations of degree greater than 0 and less than or equal to 3 of the following features: `MedInc`, `HouseAge` and `AveRooms`. If you are using PolynomialFeatures of sklearn.preprocessing make sure you drop the constant polynomial feature (degree 0). You should have a total of 19 polynomial features. \n",
    "\n",
    "Step 2: Randomly sample 80% of your data and call this the training set, and set aside the remaining 20% as your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89-Zqpssjl7U",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeXkvp7njl7U"
   },
   "source": [
    "### 4.2 Complexity and overfitting?\n",
    "\n",
    "Now, using your version of multivariate regression from 2.3, let's try to build a more complex model. **Remember to standardize appropriately!** Using the training set, regress the median house value on the polynomial features using your multivariate ols algorithm. Calculate train and test RMSE. Is this the result that you were expecting? How do these numbers compare to each other, and to the RMSE from 3.2 and nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wf3h2JEIjl7U"
   },
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDlDsi4wjl7U"
   },
   "source": [
    "### 4.3 Ridge regularization (basic)\n",
    "Incorporate L2 (Ridge) regularization into your multivariate_ols regression. Write a new version of your gradient descent algorithm that includes a regularization term \"lambda\" to penalize excessive complexity. \n",
    "\n",
    "Use your regularized regression to re-fit the model using all the polynomial features on your training data and using the value lambda = 10^4.  Report the RMSE obtained for your training data, and the RMSE obtained for your testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rlaok4imjl7U",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def multivariate_regularized_ols(xvalue_matrix, yvalues, R=0.01, MaxIterations=1000,lmbda=0):\n",
    "    start_time = time.time()\n",
    "    # Your code here\n",
    "\n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return alpha, beta_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxZdfCKvjl7U"
   },
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIYjxCfAjl7U"
   },
   "source": [
    "### 4.4: Cross-validate lambda\n",
    "\n",
    "This is where it all comes together! Use k-fold cross-validation to select the optimal value of lambda in a regression using all the polynomial features. In other words, define a set of different values of lambda. Then, using the 80% of your data that you set aside for training, iterate through the values of lambda one at a time. For each value of lambda, use k-fold cross-validation to compute the average cross-validated RMSE for that lambda value, computed as the average across the held-out folds. You should also record the average cross-validated train RMSE, computed as the average across the folds used for training. Create a scatter plot that shows RMSE as a function of lambda. The scatter plot should have two lines: a gold line showing the cross-validated RMSE, and a blue line showing the cross-validated train RMSE.  At this point, you should not have touched your held-out 20% of \"true\" test data.\n",
    "\n",
    "What value of lambda minimizes your cross-validated RMSE? Fix that value of lambda, and train a new model using all of your training data with that value of lambda (i.e., use the entire 80% of the data that you set aside in 4.1). Calculate the RMSE for this model on the 20% of \"true\" test data. How does your test RMSE compare to the RMSE from 3.2, 4.2, 4.3 and to the RMSE from nearest neighbors? What do you make of these results? \n",
    "\n",
    "Go brag to your friends about how you just implemented cross-validated ridge-regularized multivariate regression using gradient descent optimization, from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35QDSkeGjl7U",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJRo-C3kjl7U"
   },
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5: Compare your results to sklearn ridge [extra-credit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat your analysis in 4.4, but this time use the sklearn implementation of ridge regression (sklearn.linearmodel.Ridge). Are the results similar? How would you explain the differences, if any?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izzKtl3_jl7U"
   },
   "source": [
    "### 4.6: AdaGrad [extra-credit]\n",
    "\n",
    "AdaGrad is a method to implement gradient descent with different learning rates for each feature. Adaptive algorithms like this one are being extensively used especially in neural network training. Implement AdaGrad on 2.3 using `MedInc`, `HouseAge` and `AveRooms` as independent variables. Standardize these variables before inputting them to the gradient descent algorithm. Tune the algorithm until you estimate the regression coefficients within a tolerance of 1e-1. Use mini-batch gradient descent in this implementation. In summary for each parameter (in our case one intercept and three slopes) the update step of the gradient (in this example $\\beta_j$) at iteration $k$ of the GD algorithm becomes:\n",
    "\n",
    "$$\\beta_j=\\beta_j -\\frac{R}{\\sqrt{G^{(k)}_j}}\\frac{\\partial J(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j}$$ where\n",
    "$G^{(k)}_j=\\sum_{i=1}^{k} (\\frac{\\partial J^{(i)}(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j})^2$ and $R$ is your learning rate. The notation $\\frac{\\partial J^{(i)}(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j}$ corresponds to the value of the gradient at iteration $(i)$. Essentially we are \"storing\" information about previous iteration gradients. Doing that we effectively decrease the learning rate slower when a feature $x_i$ is sparse (i.e. has many zero values which would lead to zero gradients). Although this method is not necessary for our regression problem, it is good to be familiar with these methods as they are widely used in neural network training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-fxLK0Rjl7V",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAa0A7Zzjl7V"
   },
   "source": [
    "*Discuss your results here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "_INFO251-PS4 - 2022_final.ipynb",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
