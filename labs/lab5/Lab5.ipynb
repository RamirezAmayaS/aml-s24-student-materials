{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Gradient Descent\n",
    "- **Author:** Suraj R. Nair ([suraj.nair@berkeley.edu](mailto:suraj.nair@berkeley.edu)) (Based on previous material by Emily Aiken)\n",
    "- **Date:** February 14, 2024\n",
    "- **Course:** INFO 251: Applied machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics:\n",
    "1. Define a function\n",
    "2. Minimization: Closed-form solution\n",
    "3. Minimization: Naive grid search\n",
    "4. Minimization: Gradient descent\n",
    "5. Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives:\n",
    "At the end of this lab, you will understand:\n",
    "- Why gradient descent is more efficient than naive grid search\n",
    "- How to derive and code up partial derivatives for gradient descent\n",
    "- When gradient descent converges to local and global minima\n",
    "- How the number of iterations, stopping conditions, the learning rate, and random seeds impact the convergence of gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- [List of convex functions](https://web.stanford.edu/class/ee364a/lectures/functions.pdf)\n",
    "- [List of standard derivatives](https://www.mathcentre.ac.uk/resources/Engineering%20maths%20first%20aid%20kit/latexsource%20and%20diagrams/8_2.pdf)\n",
    "- [Chain rule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review)\n",
    "- [Product rule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-8/a/product-rule-review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Define a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a function with two inputs, $f(x,y)=(x + 4)^2+ y^2 + 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(x, y):\n",
    "    return (x + 4)**2 + (y)**2 + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function\n",
    "a = np.arange(-5, 5, 0.05)\n",
    "b = np.arange(-5, 5, 0.05)\n",
    "\n",
    "X, Y = np.meshgrid(a, b)\n",
    "Z = f(X, Y)\n",
    "\n",
    "sns.reset_orig()\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', linewidth=0, antialiased=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Minimization: Closed-form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What's the minimum of the function? Don't write any code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Minimization: Naive grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid to search over\n",
    "a = np.arange(-5, 5, 0.05)\n",
    "b = np.arange(-5, 5, 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "\n",
    "# Calculate the value of the function at each point in the grid\n",
    "z = f(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum value of the function\n",
    "print('Minimum value of f: %.2f' % np.min(z))\n",
    "\n",
    "# Get the arguments (x and y) at the minimum value of the function\n",
    "min_y_idx, min_x_idx = np.unravel_index(np.argmin(z, axis=None), z.shape)\n",
    "print('Arguments at minimum value of f: (%.2f, %.2f)' % (a[min_x_idx], b[min_y_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many values did we have to search over to find the minimum?\n",
    "print('Number of values tested: %i' % (len(a)*len(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Minimization: Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Calculate the *gradients* for the function. The gradients are the partial derivatives with respect to each of the inputs.\n",
    "\n",
    "$\\frac{\\partial f(x,y)}{\\partial x} =  ?$\n",
    "\n",
    "$\\frac{\\partial f(x, y)}{\\partial y} = ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let's write out these partial derivatives as functions so that we just use these throughout the notebook\n",
    "\n",
    "#### COMPLETE THIS\n",
    "\n",
    "DERIVATIVE_X = lambda x: \n",
    "\n",
    "DERIVATIVE_Y = lambda y: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(px, py, learning_rate, iterations, dx, dy):\n",
    "    \n",
    "    # Define lists to keep track of each function value visited\n",
    "    xs_visited, ys_visited, zs_visited = [px], [py], [f(px, py)]\n",
    "    \n",
    "    for i in range(iterations - 1):\n",
    "        \n",
    "        # Calculate the derivative at the point\n",
    "        # Remember to update this if you change the function above!\n",
    "        derivative_x = dx(px)\n",
    "        derivative_y = dy(py)\n",
    "        \n",
    "        # Move along the cost surface according to the derivative\n",
    "        px = px - learning_rate * derivative_x\n",
    "        py = py - learning_rate * derivative_y\n",
    "        \n",
    "        # Record the new values of the inputs (x and y) and the output (z, the function value)\n",
    "        xs_visited.append(px)\n",
    "        ys_visited.append(py)\n",
    "        zs_visited.append(f(px, py))\n",
    "    return xs_visited, ys_visited, zs_visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent for 5,000 iterations, with a learning rate of 0.001. Fix the starting points at x=1\n",
    "# and y=0.\n",
    "x_path, y_path, z_path = gradient_descent(1, 0, 0.001, 5000, DERIVATIVE_X, DERIVATIVE_Y)\n",
    "print('Minimum value of f: %.2f' % z_path[-1])\n",
    "print('Arguments at minimum value of f: (%.20f, %.20f)' % (x_path[-1], y_path[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many values did we have to visit to find the minimum?\n",
    "print('Number of values tested: %i' % (len(z_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Dig into what's happening at a few iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_slice_x(x):\n",
    "    return (x + 4)**2\n",
    "\n",
    "def f_slice_y(y):\n",
    "    return (y)**2\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "a = np.arange(-5, 5, 0.05)\n",
    "b = np.arange(-5, 5, 0.05)\n",
    "\n",
    "ax[0].plot(a, f_slice_x(a), color='darkgrey')\n",
    "ax[0].set_title('Slice of f in X direction')\n",
    "ax[0].set_xlabel('X')\n",
    "ax[0].set_ylabel('Z')\n",
    "\n",
    "ax[1].plot(b, f_slice_y(b), color='darkgrey')\n",
    "ax[1].set_title('Slice of f in Y direction')\n",
    "ax[1].set_xlabel('Y')\n",
    "ax[1].set_ylabel('Z')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point\n",
    "starting_x, starting_y = -6, 6\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.2\n",
    "\n",
    "# Calculate the derivative\n",
    "derivative_x = DERIVATIVE_X(starting_x)\n",
    "derivative_y = DERIVATIVE_Y(starting_y)\n",
    "\n",
    "# We will move along the cost surface according to the derivative\n",
    "next_x = starting_x - learning_rate * derivative_x\n",
    "next_y = starting_y - learning_rate * derivative_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "a = np.arange(-7, 7, 0.05)\n",
    "b = np.arange(-7, 7, 0.05)\n",
    "\n",
    "# Plot the slice of the function in the direction of X\n",
    "ax[0].plot(a, f_slice_x(a), color='darkgrey')\n",
    "ax[0].set_title('Slice of f in X direction')\n",
    "ax[0].set_xlabel('X')\n",
    "ax[0].set_ylabel('Z')\n",
    "\n",
    "# Plot the slice of the function in the direction of X\n",
    "ax[1].plot(b, f_slice_y(b), color='darkgrey')\n",
    "ax[1].set_title('Slice of f in Y direction')\n",
    "ax[1].set_xlabel('Y')\n",
    "ax[1].set_ylabel('Z')\n",
    "\n",
    "# Plot the starting points\n",
    "ax[0].scatter([starting_x], [f_slice_x(starting_x)], color='firebrick', s=300)\n",
    "ax[1].scatter([starting_y], [f_slice_y(starting_y)], color='firebrick', s=300)\n",
    "\n",
    "# Plot the ending points\n",
    "ax[0].scatter([next_x], [f_slice_x(next_x)], color='darkorange', s=300)\n",
    "ax[1].scatter([next_y], [f_slice_y(next_y)], color='darkorange', s=300)\n",
    "\n",
    "derivative_grid_x = np.linspace(starting_x -.5, starting_x +.5, 100)\n",
    "derivative_grid_y = np.linspace(starting_y-.5, starting_y+.5, 100)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Plot parameters as a function of the number of iterations\n",
    "Sometimes the iteration number is referred to as the \"epoch.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "ax[0].plot(range(len(x_path)), x_path, color='firebrick', linewidth=3)\n",
    "ax[0].set_title('Convergence of X')\n",
    "ax[0].set_ylabel('Value of X')\n",
    "\n",
    "ax[1].plot(range(len(y_path)), y_path, color='darkorange', linewidth=3)\n",
    "ax[1].set_title('Convergence of Y')\n",
    "ax[1].set_ylabel('Value of Y')\n",
    "\n",
    "ax[2].plot(range(len(z_path)), z_path, color='mediumseagreen', linewidth=3)\n",
    "ax[2].set_title('Convergence of Z (minimum)')\n",
    "ax[2].set_ylabel('Value of Z')\n",
    "\n",
    "for a in range(len(ax)):\n",
    "    ax[a].set_xlabel('Iteration')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Confirm that gradient descent converges no matter the random initialization\n",
    "\n",
    "\n",
    "In order to do this, let's see what happens when run gradient descent using 1000 random initializations for x and y. This might take a few seconds on your computer, so start by setting *iterations* = 1000. \n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with random initializations for gradient descent\n",
    "x_mins, y_mins, z_mins = [], [], []\n",
    "for i in range(1000):\n",
    "    x_path, y_path, z_path = gradient_descent(np.random.rand()*2 - 1, np.random.rand()*2 - 1, 0.001, 1000, DERIVATIVE_X, DERIVATIVE_Y)\n",
    "    x_mins.append(x_path[-1])\n",
    "    y_mins.append(y_path[-1])\n",
    "    z_mins.append(z_path[-1])\n",
    "    \n",
    "sns.set(font_scale=2)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "ax[0].hist(x_mins, color='firebrick', alpha=0.2, bins=20, edgecolor='firebrick', linewidth=2)\n",
    "ax[0].axvline(np.mean(x_mins), dashes=[1, 1], color='firebrick', linewidth=4)\n",
    "ax[0].set_title('Distribution of X Value for Minimum')\n",
    "ax[0].set_xlabel('X value (converged)')\n",
    "\n",
    "ax[1].hist(y_mins, color='darkorange', alpha=0.2, bins=20, edgecolor='darkorange', linewidth=2)\n",
    "ax[1].axvline(np.mean(y_mins), dashes=[1, 1], color='darkorange', linewidth=4)\n",
    "ax[1].set_title('Distribution of Y Value for Minimum')\n",
    "ax[1].set_xlabel('Y value (converged)')\n",
    "\n",
    "ax[2].hist(z_mins, color='mediumseagreen', alpha=0.2, bins=20, edgecolor='mediumseagreen', linewidth=2)\n",
    "ax[2].axvline(np.mean(z_mins), dashes=[1, 1], color='mediumseagreen', linewidth=4)\n",
    "ax[2].set_title('Distribution of Minimum')\n",
    "ax[2].set_xlabel('Minimum (converged)')\n",
    "\n",
    "for a in range(len(ax)):\n",
    "    ax[a].set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the learning rate impact the convergence of gradient descent?\n",
    "min_xs, min_ys, min_zs = [], [], []\n",
    "learning_rate_grid = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001, 0.000000001]\n",
    "for learning_rate in learning_rate_grid:\n",
    "    x_path, y_path, z_path = gradient_descent(1, 0, learning_rate, 1000, DERIVATIVE_X, DERIVATIVE_Y)\n",
    "    min_xs.append(x_path[-1])\n",
    "    min_ys.append(y_path[-1])\n",
    "    min_zs.append(z_path[-1])\n",
    "    \n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "ax[0].scatter(learning_rate_grid, min_xs, s=100, color='firebrick')\n",
    "ax[0].set_title('Convergence of X based on learning rate')\n",
    "ax[0].set_ylabel('X at minimum')\n",
    "\n",
    "ax[1].scatter(learning_rate_grid, min_ys, s=100, color='darkorange')\n",
    "ax[1].set_title('Convergence of Y based on learning rate')\n",
    "ax[1].set_ylabel('Y at minimum')\n",
    "\n",
    "ax[2].scatter(learning_rate_grid, min_zs, s=100, color='mediumseagreen')\n",
    "ax[2].set_title('Convergence of Z based on learning rate')\n",
    "ax[2].set_ylabel('Minimum (Z)')\n",
    "\n",
    "for a in range(len(ax)):\n",
    "    ax[a].set_xscale('log')\n",
    "    ax[a].set_xlabel('Learning Rate')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Stopping conditions\n",
    "One option is to set a number of iterations, and confirm that gradient descent has converged. Another option is to set a *stopping condition*, where we'll stop running gradient descent if *k* iterations in the row all have approximately the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_w_stopping(px, py, learning_rate, dx, dy, stopping_tolerance):\n",
    "    \n",
    "    # Define lists to keep track of each function value visited\n",
    "    xs_visited, ys_visited, zs_visited = [np.inf, px], [np.inf, py], [np.inf, f(px, py)]\n",
    "    num_iter = 0\n",
    "    while np.abs(xs_visited[-2] - xs_visited[-1]) > stopping_tolerance:\n",
    "        \n",
    "        # Calculate the derivative at the point\n",
    "        derivative_x = dx(px)\n",
    "        derivative_y = dy(py)\n",
    "        \n",
    "        # Move along the cost surface according to the derivative\n",
    "        px = px - learning_rate * derivative_x\n",
    "        py = py - learning_rate * derivative_y\n",
    "        \n",
    "        # Record the new values of the inputs (x and y) and the output (z, the function value)\n",
    "        xs_visited.append(px)\n",
    "        ys_visited.append(py)\n",
    "        zs_visited.append(f(px, py))\n",
    "        num_iter +=1\n",
    "    \n",
    "    return xs_visited, ys_visited, zs_visited, num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent for 10000 iterations, with a learning rate of 0.001. Fix the starting points at x=1\n",
    "# and y=0.\n",
    "x_path, y_path, z_path = gradient_descent(1, 0, 0.001, 10000,  DERIVATIVE_X, DERIVATIVE_Y)\n",
    "print('Minimum value of f: %.2f' % z_path[-1])\n",
    "print('Arguments at minimum value of f: (%.20f, %.20f)' % (x_path[-1], y_path[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the tolerance of the stopping criterion impact the convergence of gradient descent?\n",
    " \n",
    "Run the code below, and consider the following questions:\n",
    "\n",
    "- At what level(s) of tolerance does gradient descent fail to converge for x and y, respectively?\n",
    "- What is the relationship between the tolerance of the stopping criteria, and the number of iterations gradient descent takes to converge? (draw a plot to demonstrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_xs, min_ys, min_zs, num_iters = [], [], [], []\n",
    "tolerance_grid = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001, 0.000000001]\n",
    "for stopping_tolerance in tolerance_grid:\n",
    "    x_path, y_path, z_path, num_iter = gradient_descent_w_stopping(1, 0, 0.001, DERIVATIVE_X, DERIVATIVE_Y, stopping_tolerance)\n",
    "    min_xs.append(x_path[-1])\n",
    "    min_ys.append(y_path[-1])\n",
    "    min_zs.append(z_path[-1])\n",
    "    num_iters.append(num_iter)\n",
    "    \n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "ax[0].scatter(tolerance_grid, min_xs, s=100, color='firebrick')\n",
    "ax[0].set_title('Convergence of X based on stopping criterion')\n",
    "ax[0].set_ylabel('X at minimum')\n",
    "\n",
    "ax[1].scatter(tolerance_grid, min_ys, s=100, color='darkorange')\n",
    "ax[1].set_title('Convergence of Y based on stopping criterion')\n",
    "ax[1].set_ylabel('Y at minimum')\n",
    "\n",
    "ax[2].scatter(tolerance_grid, min_zs, s=100, color='mediumseagreen')\n",
    "ax[2].set_title('Convergence of Z based on stopping criterion')\n",
    "ax[2].set_ylabel('Minimum (Z)')\n",
    "\n",
    "for a in range(len(ax)):\n",
    "    ax[a].set_xlabel('Stopping Tolerance')\n",
    "    ax[a].set_xscale('log')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Your turn!\n",
    "\n",
    "This time, you'll work with the following function:\n",
    "\n",
    "$f(x,y) = x~ln(x^2) + y^4 + 1$.\n",
    "\n",
    "Reminder: The default numpy logarithm (np.log) is a natural logarithm.\n",
    "\n",
    "Reminder: The natural logarithm is only defined for x > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(x, y):\n",
    "    fxy = x * np.log(x**2) + y**4 + 1\n",
    "    return fxy\n",
    "\n",
    "# Define the partial derivatives\n",
    "\n",
    "DERIVATIVE_X = lambda x: 2 + np.log(x**2)\n",
    "\n",
    "DERIVATIVE_Y = lambda y: 4 * (y**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function\n",
    "a = np.arange(-6, 6, 0.05)\n",
    "b = np.arange(-6, 6, 0.05)\n",
    "\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = f(x, y)\n",
    "\n",
    "sns.reset_orig()\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x, y, z, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try minimizing with naive grid search\n",
    "a = np.arange(-6, 6, 0.05)\n",
    "b = np.arange(-6, 6, 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = f(x, y)\n",
    "\n",
    "# Find the minimum value of the function\n",
    "print('Minimum value of f: %.2f' % np.min(z))\n",
    "\n",
    "# Get the arguments (x and y) at the minimum value of the function\n",
    "min_y_idx, min_x_idx = np.unravel_index(np.argmin(z, axis=None), z.shape)\n",
    "print('Arguments at minimum value of f: (%.2f, %.2f)' % (a[min_x_idx], b[min_y_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What are the partial derivatives of the function?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$\\frac{\\partial f(x,y)}{\\partial x} =  ?$\n",
    "\n",
    "$\\frac{\\partial f(x, y)}{\\partial y} = ?$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use gradient descent to minimize the function.\n",
    "\n",
    "p, u, i = gradient_descent(-10, -10,  0.00001, 2350, DERIVATIVE_X, DERIVATIVE_Y)\n",
    "\n",
    "print(p[-1], u[-1], i[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
